<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    <title>Tiiktak&#39;s</title>
    
    <meta name="description" content="Hey You">
    <meta name="author" content="">
    
    <link href="https://konosuba.xyz/css/github-gist.min.css" rel="stylesheet">
    <link href="https://konosuba.xyz/css/style.css" rel="stylesheet">
    
    <link rel="apple-touch-icon" href="https://konosuba.xyz/img/apple-touch-icon.png">
    <link rel="icon" href="https://konosuba.xyz/img/favicon.ico">
    
    <meta name="generator" content="Hugo 0.55.6" />
    
    <link rel="alternate" type="application/atom+xml" href="https://konosuba.xyz/index.xml" title="Tiiktak&#39;s">
    
    
    
  </head>
  <body class="list home">
    <header class="header">
      <div class="wrap">
        
        <h1 class="logo"><a href="https://konosuba.xyz/">Tiiktak&#39;s</a></h1>
        
        
        <button class="menu-toggle" type="button"></button>
        
      </div>
    </header>
    
    <nav class="nav">
      <ul class="menu">
        
        <li>
          <a href="/about/">About</a>
        </li>
        
      </ul>
    </nav>
    
    <main class="main">








<article class="first-entry">
  <header class="entry-header">
    <h2>About</h2>
  </header>
  <section class="entry-content">
   <p>:D 获取中…
…
fetch(&#39;https://v1.hitokoto.cn/?c=a&#39;) .then(function (res){ return res.json(); }) .then(function (data) { var hitokoto = document.getElementById(&#39;hitokoto&#39;); var hitokotofrom = document.getElementById(&#39;from&#39;); hitokoto.innerText = data.hitokoto; hitokotofrom.innerText = data.from; }) .catch(function (err) { console.error(err); })  WHO AM I 西南地区某双非大学的在读本科生一枚
视觉检测研究实验室成员
MSP微软校园精英成员
热爱平面设计，还对广告文案有浓厚兴趣
喜欢日本文化（人生第一次出国旅行规划ing）
Archives 基于OpenCV的答题卡识别
基于OpenCV的身份证识别
基于树莓派的视频流传输及灰度重心识别
基于OpenCV的MLS图像扭曲变形实现
（都是些渣渣项目，会努力让这里变得好看的😍）
Contact Me  QQ: 1067475613
 Mail: dengsh268@outlook.com
    (adsbygoogle = window.adsbygoogle || []).push({ google_ad_client: &#34;ca-pub-9673215637005333&#34;, enable_page_level_ads: true });    (adsbygoogle = window....</p>
  </section>
  <footer class="entry-footer">
    <time>2019.4.9</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/about/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>MMDet中使用Mask-RCNN训练BDD100K数据集</h2>
  </header>
  <section class="entry-content">
   <p>1. Convert label to COCO format 使用官方提供的工具bdd100k2coco.py
 注意：使用bdd100k2coco分支下的文件，master中的该文件无法正常使用
 在使用前，还需要对该文件进行部分修改：
# 将69行修改为	image[&#34;file_name&#34;] = frame[&#34;name&#34;]&#43;&#34;.jpg&#34; # 在73行之后增加 frame = frame[&#34;frames&#34;][0]  之后按如下方式运行
python bdd100k2coco.py -i {JSON文件夹路径} -o {输出一个coco json文件} -m {转换方式det or track，使用det即可}  调用两次分别将train、val数据集的label转换
2. Set COCO-like directory tree 按如下目录结构保存我们的数据集
data ├── annotations │ ├── instances_train2017.json # 训练集json │ ├── instances_val2017.json	# 验证集json ├── train2017 │ └── abcdefg-1234567.jpg │ └── ... ├── test2017 │ └── abcdefg-1234567.jpg │ └── ....</p>
  </section>
  <footer class="entry-footer">
    <time>2020.10.8</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/mmdet%E4%B8%AD%E4%BD%BF%E7%94%A8mask-rcnn%E8%AE%AD%E7%BB%83bdd100k%E6%95%B0%E6%8D%AE%E9%9B%86/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>MMDet安装过程记录</h2>
  </header>
  <section class="entry-content">
   <p>记录自本人安装过程，环境建议：
 Linux or macOS (Windows is not currently officially supported) Python 3.6&#43; PyTorch 1.3&#43; CUDA 9.2&#43; (If you build PyTorch from source, CUDA 9.0 is also compatible) GCC 5&#43;  1. torch &amp; torchvision 我使用的是 torch 1.5.1 &#43; torchvision 0.6.1
pip install torch==1.5.1 torchvision==0.6.1 [-i https://pypi.douban.com/simple]
 https://pytorch.org/get-started/previous-versions/
 2. mmcv pip install mmcv-full
3. mmdetection git clone https://github.com/open-mmlab/mmdetection.git cd mmdetection pip install -r requirements/build.txt pip install -v -e ....</p>
  </section>
  <footer class="entry-footer">
    <time>2020.10.5</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/mmdet%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>查看当前使用的Python的安装路径</h2>
  </header>
  <section class="entry-content">
   <p>如题，代码如下：
import sys python_path = sys.executable print(python_path)  如图：...</p>
  </section>
  <footer class="entry-footer">
    <time>2020.9.25</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E4%BD%BF%E7%94%A8%E7%9A%84python%E7%9A%84%E5%AE%89%E8%A3%85%E8%B7%AF%E5%BE%84/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>python库opencv,py-opencv,libopencv的区别</h2>
  </header>
  <section class="entry-content">
   <p>通常我们在Python中安装OpenCV都是直接用pip install opencv-python
今天想用Anaconda Navigator安装的时候，在面板中搜索到有libopencv, opencv, py-opencv共三个包，而且三者的描述都是同样的’Computer vision and machine learning software library‘，瞬间迷惑:laughing:
找到介绍如下：
 OpenCV is computer vision a library written using highly optimized C/C&#43;&#43; code. It makes use of multiprocessing in the background. It has a collection of a large number of algorithms tested and verifiend by the developers. The best thing about this is it’s FREE under the BSD license. libopencv is only a metapackage. These packages do not contain actual software, they simply depend on other packages to be installed....</p>
  </section>
  <footer class="entry-footer">
    <time>2020.7.7</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/python%E5%BA%93opencvpy-opencvlibopencv%E7%9A%84%E5%8C%BA%E5%88%AB/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>树莓派4B安装opencv以及错误解决</h2>
  </header>
  <section class="entry-content">
   <p>更新于2020/4/27 更新：换了个树莓派4B，安装opencv的时候遇到了一些之前没碰到的问题，在这里记录一下
 主要参考opencv官网文档和博客树莓派&#43;Opencv（一）图像处理
树莓派4B上安装参考：树莓派4B 安装opencv完整教程基于python3（各种错误解决）
下载安装依赖项 sudo apt-get install build-essential sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev  在4B上安装时遇到libgtk2.0-dev安装失败的问题：
这是因为依赖项版本太高了，需要降级安装。所以可以使用命令sudo aptitude install libgtk2.0-dev来进行安装
在安装过程中，首先会给出一个方案提示是否接受，第一个给出的方案是保留原依赖项，我们要输入n否定它，之后给出第二个方案是降级安装，输入Y使用该方案
 其中aptitude是一个类似apt-get的包管理工具，但是它能更好处理依赖问题，支持降级安装
 下载源码 从GitHub下载：
opencv
opencv_contrib
两个都下载.zip压缩包即可
解压源码并进入文件夹 unzip opencv-4.3.0.zip unzip opencv_contrib-4.3.0.zip cd opencv-4.3.0  创建一个build文件夹用于编译 mkdir build cd build  运行cmake-gui  这一步其实也可以直接使用cmake配合各类参数，不过我觉得图形化界面方便一点
 cmake-gui  选择源码路径和编译路径后点击Configure...</p>
  </section>
  <footer class="entry-footer">
    <time>2020.4.27</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/%E6%A0%91%E8%8E%93%E6%B4%BE4b%E5%AE%89%E8%A3%85opencv%E4%BB%A5%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>AlexNet分类Fashion-MNIST(Pytorch实现)</h2>
  </header>
  <section class="entry-content">
   <p>这个notebook也同时发表在Kaggle上
Fashion MNIST数据集 |Label | Class | |-|-| |0| T-shirt/top| |1| Trouser| |2| Pullover| |3| Dress| |4| Coat| |5| Sandal| |6| Shirt| |7| Sneaker| |8| Bag| |9| Ankle boot|
准备工作 import os import torch import torch.nn.functional as F import torch.nn as nn import torch.optim as optim import numpy as np import pandas as pd from PIL import Image import matplotlib.pyplot as plt from torchvision import transforms, datasets from torch.utils.data import Dataset, DataLoader EPOCHS = 20 BATCH_SIZE = 512 DEVICE = (&#34;cuda&#34; if torch....</p>
  </section>
  <footer class="entry-footer">
    <time>2020.3.8</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/alexnet%E5%88%86%E7%B1%BBfashion-mnistpytorch%E5%AE%9E%E7%8E%B0/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>循环神经网络RNN以及几种经典模型</h2>
  </header>
  <section class="entry-content">
   <p>RNN简介 现实世界中，很多元素都是相互连接的，比如室外的温度是随着气候的变化而周期性的变化的、我们的语言也需要通过上下文的关系来确认所表达的含义。但是机器要做到这一步就相当得难了。因此，就有了现在的循环神经网络，他的本质是：*拥有记忆的能力，并且会根据这些记忆的内容来进行推断*。因此，他的输出就依赖于当前的输入和记忆。
网络结构及原理 循环神经网络的基本结构特别简单，就是将网络的输出保存在一个记忆单元中，这个记忆单元和下一次的输入一起进入神经网络中。
一个最简单的循环神经网络在输入时的结构示意图：
RNN 可以被看做是同一神经网络的多次赋值，每个神经网络模块会把消息传递给下一个，我们将这个图的结构展开:
根据循环神经网络的结构也可以看出它在处理序列类型的数据上具有天然的优势。因为网络本身就是 一个序列结构，这也是所有循环神经网络最本质的结构。
我们可以用下面的公式来表示循环神经网络的计算方法：
总结图：
Pytorch中 pytorch 中使用 nn.RNN 类来搭建基于序列的循环神经网络，它的构造函数有以下几个参数：
 input_size：输入数据X的特征值的数目。 hidden_size：隐藏层的神经元数量，也就是隐藏层的特征数量。 num_layers：循环神经网络的层数，默认值是 1。 bias：默认为 True，如果为 false 则表示神经元不使用 bias 偏移参数。 batch_first：如果设置为 True，则输入数据的维度中第一个维度就是 batch 值，默认为 False。默认情况下第一个维度是序列的长度， 第二个维度才是batch，第三个维度是特征数目。 dropout：如果不为空，则表示最后跟一个 dropout 层抛弃部分数据，抛弃数据的比例由该参数指定  RNN 中最主要的参数是 input_size 和 hidden_size，这两个参数务必要搞清楚。其余的参数通常不用设置，采用默认值就可以了。
rnn = torch.nn.RNN(20,50,2) input = torch.randn(100 , 32 , 20) h_0 =torch.randn(2 , 32 , 50) output,hn=rnn(input ,h_0) print(output.size(),hn.size()) &#39;&#39;&#39; torch.Size([100, 32, 50]) torch.Size([2, 32, 50]) &#39;&#39;&#39;   一文搞懂RNN（循环神经网络）基础篇...</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.19</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn%E4%BB%A5%E5%8F%8A%E5%87%A0%E7%A7%8D%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>卷积神经网络CNN以及几种经典模型</h2>
  </header>
  <section class="entry-content">
   <p>简介  CNN -&gt; Convolutional Neural Network
 卷积神经网络是由一个或多个卷积层和顶端的全连通层（也可以使用1x1的卷积层作为最终的输出）组成的一种前馈神经网络
基本概念 局部感受野(Local Receptive Fields) 一般的神经网络往往会把图像的每一个像素点连接到全连接的每一个神经元中，而卷积神经网络则是把每一个隐藏节点只连接到图像的某个局部区域，从而减少参数训练的数量。
例如，一张1024×720的图像，使用9×9的感受野，则只需要81个权值参数。对于一般的视觉也是如此，当观看一张图像时，更多的时候关注的是局部。
共享权值(Shared Weights) 在卷积神经网络的卷积层中，神经元对应的权值是相同的，由于权值相同，因此可以减少训练的参数量。共享的权值和偏置也被称作*卷积核*或滤波器
池化(Pooling) 由于待处理的图像往往都较大，而实际处理时没必要直接对原图进行分析，最主要的是要能够有效获得图像的特征。因此可以采用类似图像压缩的思想，对图像进行卷积之后，通过一个下采样过程来调整图像的大小
 什么是下采样？ 上采样、下采样到底是个啥
 结构组成 我们通过卷积的计算操作来*提取图像局部的特征*，每一层都会计算出一些局部特征，这些局部特征再汇总到下一层，这样*一层一层的传递*下去，特征由小变大，最后在通过这些局部的特征对图片进行处理，这样大大提高了计算效率，也提高了准确度。
卷积层 提取特征
卷积计算  动图来源于：stanford.edu, Feature extraction using convolution
NOTE: 深度学习中的卷积与信号处理中的卷积略有不同，深度学习中的卷积略去了翻转的步骤（因为起初卷积核是随机生成的，没有方向）
  输入矩阵大小 n 卷积核大小 f 边界填充 (p)adding，指在原矩阵周围填充的层数 步长 (s)tride  计算公式
卷积结果大小：(n - f &#43; 2p) / s &#43; 1向下取整
多个卷积核 在每一个卷积层我们会设置多个卷积核，代表多个不同的特征，这些特征就是需要传递到下一层的输出，训练的过程就是训练不同的核
激活函数 引入非线性关系
由于卷积的操作是线性的，所以需要使用进行激活，通常使用Relu
池化层 减少参数数量
通过减少卷积层之间的连接，降低运算复杂程度。
池化层一般放在卷积层后面，所以池化层池化的是卷积层的输出
一般使用的有最大池化max-pooling和平均池化mean-pooling
操作与卷积类似，即过滤器在矩阵上滑动...</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.18</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ccnn%E4%BB%A5%E5%8F%8A%E5%87%A0%E7%A7%8D%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>Pytorch中的激活函数</h2>
  </header>
  <section class="entry-content">
   <p>介绍神经网络的时候已经说到，神经元会对化学物质的刺激进行，当达到一定程度的时候，神经元才会兴奋，并向其他神经元发送信息。神经网络中的激活函数就是用来判断我们所计算的信息是否达到了往后面传输的条件。
为什么激活函数都是非线性的 因为如果使用线性的激活函数，那么input跟output之间的关系始终为线性的，这样完全可以不使用网络结构，直接使用线性组合即可。
所以需要激活函数来引入非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中，增加了神经网络模型泛化的特性。
一般只有在输出层有极小的可能性使用线性激活函数，在隐含层都使用非线性激活函数.
常见的激活函数 # 初始化一些信息 import torch import torch.nn.functional as F import matplotlib.pyplot as plt import numpy as np x= torch.linspace(-10,10,60)  Sigmoid 函数 g(z) = a = 1 / (1 &#43; e^(-z)) g&#39;(z) = a&#39; = a (1 - a)  在sigmod函数中我们可以看到，其输出是在(0,1)这个开区间，它能够把输入的连续实值变换为0和1之间的输出，如果是非常大的负数，那么输出就是0；如果是非常大的正数输出就是1，起到了抑制的作用。
ax = plt.gca() ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.xaxis.set_ticks_position(&#39;bottom&#39;) ax.spines[&#39;bottom&#39;].set_position((&#39;data&#39;, 0)) ax.yaxis.set_ticks_position(&#39;left&#39;) ax.spines[&#39;left&#39;].set_position((&#39;data&#39;, 0)) plt.ylim((0, 1)) sigmod=torch.sigmoid(x) plt.plot(x.numpy(),sigmod.numpy())  但是sigmod由于需要进行指数运算（这个对于计算机来说是比较慢，相比relu），再加上函数输出*不是以0为中心*的（这样会使权重更新效率降低），当输入稍微远离了坐标原点，函数的梯度就变得很小了（几乎为零）。
在神经网络反向传播的过程中不利于权重的优化，这个问题叫做梯度饱和，也可以叫梯度弥散。这些不足，所以现在使用到sigmod基本很少了，基本上只有在做二元分类（0，1）时的输出层才会使用。
Tanh 函数 tanh是双曲正切函数，输出区间是在(-1,1)之间，而且整个函数是*以0为中心*的
ax = plt....</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.15</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/pytorch%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"></a>
</article>




<footer class="page-footer">
  <nav class="pagination">
    
    
    <a class="next" href="/page/2/">Next Page →</a>
    
  </nav>
</footer>


</main>
<footer class="footer">
  <span>&copy; 2020 <a href="https://konosuba.xyz/">Tiiktak&#39;s</a></span>
  <span>&middot;</span>
  <span>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</span>
  <span>&middot;</span>
  <span>Theme️ <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper</a></span>
</footer>
<script src="https://konosuba.xyz/js/instantclick.min.js" data-no-instant></script>
<script data-no-instant>InstantClick.init();</script>
<script src="https://konosuba.xyz/js/highlight.min.js" data-no-instant></script>
<script data-no-instant>
  let body;
  function menuToggleListener() {
    body.classList.toggle('blur');
  }
  function setMenuToggleListener() {
    const menuToggle = document.querySelector('.menu-toggle');
    if (!menuToggle) return;
    body = document.querySelector('body');
    menuToggle.addEventListener('click', menuToggleListener);
  }

  hljs.initHighlightingOnLoad();
  setMenuToggleListener();

  InstantClick.on('change', function () {
    document.querySelectorAll('pre code').forEach((block) => {
      hljs.highlightBlock(block);
    });
    setMenuToggleListener();
  });
</script>
</body>
</html>

