<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    <title>Posts - Tiiktak&#39;s</title>
    
    <meta name="description" content="Hey You">
    <meta name="author" content="">
    
    <link href="https://konosuba.xyz/css/github-gist.min.css" rel="stylesheet">
    <link href="https://konosuba.xyz/css/style.css" rel="stylesheet">
    
    <link rel="apple-touch-icon" href="https://konosuba.xyz/img/apple-touch-icon.png">
    <link rel="icon" href="https://konosuba.xyz/img/favicon.ico">
    
    <meta name="generator" content="Hugo 0.55.6" />
    
    <link rel="alternate" type="application/atom+xml" href="https://konosuba.xyz/index.xml" title="Tiiktak&#39;s">
    
    
    
  </head>
  <body class="list">
    <header class="header">
      <div class="wrap">
        
        <p class="logo"><a href="https://konosuba.xyz/">Tiiktak&#39;s</a></p>
        
        
        <button class="menu-toggle" type="button"></button>
        
      </div>
    </header>
    
    <nav class="nav">
      <ul class="menu">
        
        <li>
          <a href="/about/">About</a>
        </li>
        
      </ul>
    </nav>
    
    <main class="main">



<header class="page-header">
  <h1>
  Posts
  </h1>
</header>






<article class="post-entry">
  <header class="entry-header">
    <h2>Pytorch_linear</h2>
  </header>
  <section class="entry-content">
   <p> Linear 对输入数据应用线性变换：y = xA^T &#43; b
torch.nn.Linear(in_features, out_features, bias=True)  参数  in_features 每个输入样本的大小 out_features 每个输出样本的大小 bias 若为False，layer不会学习附加偏差b  shape  输入: (N, ∗, H_in)，其中 ∗ 代表任意数量的附加维度，H_in = in_features
 输出: (N, *, H_out)，除了最后一个维度，其余都与输入相同，H_out = out_features
  ...</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.12</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/pytorch_linear/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>Pytorch_nn.Conv2d</h2>
  </header>
  <section class="entry-content">
   <p> Conv2d torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=&#39;zeros&#39;)   in_channels 输入数据通道数 out_channels 输出数据通道数 kennel_size 卷积核大小，int或tuple stride 步长 padding 每个维度零填充的数量 dilation 内核点之间的距离，也称à trous algorithm groups 控制inputs与outputs间的连接 &gt; * groups=1，所有输入都卷积到输出 &gt; * groups=2，并排设置两个conv层，每个层查看一半的输入通道，并生成一半的输出通道，然后将两者连接起来 &gt; * groups=in_channels，每个输入通道都有它自己的filter，size为[out_channels/in_channels]  ...</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.12</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/pytorch_nn.conv2d/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>Python面向对象_super()函数</h2>
  </header>
  <section class="entry-content">
   <p>super() super()是用于调用父类（超类）的一个方法
用于解决多重继承问题：直接用类名调用父类方法在使用单继承时没有问题，但若使用多继承，会涉及到查找顺序（MRO）、重复调用（钻石继承）等问题
 MRO 就是类的方法解析顺序表, 其实也就是继承父类方法时的顺序表。
 语法 super(type[, object-or-type])  参数  type 类
 object-or-type 类，一般是self
  在 Python3 中：super().xxx
在 Python2 中：super(Class, self).xxx
两者等价
示例 class Bird: def __init__(self): self.hungry = True def eat(self): if self.hungry: print(&#39;Ahahahah&#39;) else: print(&#39;No thanks!&#39;) class SongBird(Bird): def __init__(self): self.sound = &#39;Squawk&#39; def sing(self): print(self.sound) sb = SongBird() sb.sing() # 能正常输出&#39;Squawk&#39; sb.eat() # 报错，因为 SongBird 中没有 hungry 特性  使用super解决：...</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.12</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/python%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1_super%E5%87%BD%E6%95%B0/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>Pytorch学习笔记_3_构建一个神经网络</h2>
  </header>
  <section class="entry-content">
   <p>Neural Networks  神经网络可以通过使用torch.nn包来创建
 nn依赖于autograd来定义模型并求导。
 一个nn.Module类包含各个层和一个forward(input)前向传播方法，该方法返回output
  例如这个分类数字图像的网络：
这是个简单的前馈神经网络，它接受一个输入，然后一层接一层的传递，最后输出计算结果
一个神经网络的典型训练过程：
 定义包含一些可学习的参数（或权重）的神经网络 在数据集上迭代 通过神经网络处理输入 计算损失函数（预测值与实际值的差值大小） 将梯度反向传播回网络的参数 更新网络参数，主要使用一个简单的更新法则：weight = weight - learning_rate * gradient   另参见：konosuba.xyz/blog/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4
 定义网络 import torch import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): # 构造方法 super().__init__() # 复制并使用Net的父类的初始化方法，即先运行nn.Module的初始化函数 # 卷积层 self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # fc(full_connect)全连接函数，均为线性函数 y = Wx &#43; b self....</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.12</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_3_%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>Pytorch学习笔记_2_Autograd自动求导机制</h2>
  </header>
  <section class="entry-content">
   <p>Autograd 自动求导机制 PyTorch 中所有神经网络的核心是 autograd 包。
autograd 包为张量上的所有操作提供了自动求导。它是一个在运行时定义的框架，可以通过代码的运行来决定反向传播的过程，并且每次迭代可以是不同的。
通过一些示例来了解
Tensor 张量 torch.tensor是这个包的核心类。
 设置.requires_grad为True，会追踪所有对于该张量的操作。计算完成后调用.backward()，可以自动计算所有的梯度，并自动累计到.grad属性中   事实上即使.requires_grad为True并不意味着.grad一定不为None
  可以调用.detach()将该张量与计算历史记录分离，并禁止跟踪它将来的计算记录
 为防止跟踪历史记录（和使用内存），可以将代码块包装在with torch.no_grad():中。这在评估模型时特别有用，因为模型可能具有requires_grad = True的可训练参数，但是我们不需要梯度计算。
  Function类 Tensor 和 Function 互相连接并生成一个非循环图，它表示和存储了完整的计算历史。
每个张量都有一个.grad_fn属性，对张量进行操作后，grad_fn会引用一个创建了这个Tensor类的Function对象（除非这个张量是用户手动创建的，此时，这个张量的 grad_fn 是 None）
 leaf Tensors 叶张量
Tensor中有一属性is_leaf，当它为True有两种情况：
 按照惯例，requires_grad = False 的 Tensor
 requires_grad = True 且由用户创建的 Tensor。这意味着它们不是操作的结果且grad_fn = None
  只有leaf Tensors叶张量在反向传播时才会将本身的grad传入backward()的运算中。要想得到non-leaf Tensors非叶张量在反向传播时的grad，可以使用retain_grad()
 如果需要计算导数，可以在Tensor上调用.backward()：若Tensor是一个标量（即包含一个元素数据）则不需要为backward()指定任何参数， 但是如果它有更多的元素，需要指定一个gradient 参数来匹配张量的形状。
x = torch.ones(2, 2, requires_grad=True) print(x) # Output: # tensor([[1....</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.11</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_2_autograd%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E6%9C%BA%E5%88%B6/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>Pytorch学习笔记_1_tensor张量</h2>
  </header>
  <section class="entry-content">
   <p>Tensors Tensors与Numpy中的ndarrays类似
torch.new_* 与 torch.*_like 前者创建的对象会保持原有的属性（如dtype），但shape不同
&gt;&gt;&gt; x = torch.zeros(5, 3, dtype=torch.double) &gt;&gt;&gt; x.new_ones(2, 3) tensor([[1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) &gt;&gt;&gt; x.new_ones(2, 3, dtype=torch.long) tensor([[1, 1, 1], [1, 1, 1]])  后者可以创建shape相同，属性不同的对象
&gt;&gt;&gt; x = torch.zeros(5, 3, dtype=torch.double) &gt;&gt;&gt; torch.ones_like(x) tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) &gt;&gt;&gt; torch.ones_like(x, dtype=torch.long) tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]])  获得size 使用size方法与Numpy的shape属性返回的相同，张量也支持shape属性...</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.11</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_1_tensor%E5%BC%A0%E9%87%8F/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>神经网络模型的学习曲线</h2>
  </header>
  <section class="entry-content">
   <p> 学习曲线：样本数量与误差 绘制 样本数量m 与 训练误差、交叉验证误差 的关系曲线
高偏差（欠拟合）high bias 高方差（过拟合）high variance ...</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.7</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>【应用机器学习】正则化与偏差、方差</h2>
  </header>
  <section class="entry-content">
   <p> 在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。
但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与之前选择多项式模型次数类似的问题。
我们选择一系列的想要测试的λ值，比如这里选择 0-10之间的值，通常呈现2倍关系（如：0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10 共12个）。
我们同样把数据分为训练集、交叉验证集和测试集。
选择λ的方法  使用训练集训练出12个不同程度正则化的模型
 用12个模型分别对交叉验证集计算的出交叉验证误差
 选择得出交叉验证误差最小的模型
 运用步骤3中选出模型对测试集计算得出推广误差
  我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上：
 当λ较小时，训练集误差较小（过拟合）而交叉验证集误差较大
随着λ的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加
 ...</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.7</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>【应用机器学习】诊断偏差与方差</h2>
  </header>
  <section class="entry-content">
   <p>当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。
我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析：
 对于训练集，当 d 较小时，模型拟合程度更低，误差较大；随着d 的增长，拟合程度提高，误差减小。
 对于交叉验证集，当 d 较小时，模型拟合程度低，误差较大；但是随着 d 的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。
  判断高偏差（欠拟合）或高方差（过拟合）  训练集误差和交叉验证集误差近似时：偏差/欠拟合
 ​交叉验证集误差远大于训练集误差时：方差/过拟合
  解决欠拟合与过拟合 欠拟合： - 增加网络结构，如增加隐藏层数目； - 训练更长时间； - 寻找合适的网络架构，使用更大的NN结构；
过拟合 ： - 使用更多的数据； - 正则化（ regularization）； - 寻找合适的网络结构；...</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.7</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%8A%E6%96%AD%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/"></a>
</article>

<article class="post-entry">
  <header class="entry-header">
    <h2>【应用机器学习】模型选择和训练、验证、测试集</h2>
  </header>
  <section class="entry-content">
   <p> 1. 重新划分数据集 其中60%作为训练集，20%作为交叉验证集（cross validation），20%作为测试集
2. 可以计算出三类数据的误差函数 3. 使用交叉验证集选择模型 选出交叉验证误差最小的一个模型
4. 利用测试集计算出推广误差 ...</p>
  </section>
  <footer class="entry-footer">
    <time>2020.2.7</time>
  </footer>
  <a class="entry-link" href="https://konosuba.xyz/blog/%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E5%92%8C%E8%AE%AD%E7%BB%83%E9%AA%8C%E8%AF%81%E6%B5%8B%E8%AF%95%E9%9B%86/"></a>
</article>




<footer class="page-footer">
  <nav class="pagination">
    
    <a class="prev" href="/blog/">← Prev Page</a>
    
    
    <a class="next" href="/blog/page/3/">Next Page →</a>
    
  </nav>
</footer>


</main>
<footer class="footer">
  <span>&copy; 2020 <a href="https://konosuba.xyz/">Tiiktak&#39;s</a></span>
  <span>&middot;</span>
  <span>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</span>
  <span>&middot;</span>
  <span>Theme️ <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper</a></span>
</footer>
<script src="https://konosuba.xyz/js/instantclick.min.js" data-no-instant></script>
<script data-no-instant>InstantClick.init();</script>
<script src="https://konosuba.xyz/js/highlight.min.js" data-no-instant></script>
<script data-no-instant>
  let body;
  function menuToggleListener() {
    body.classList.toggle('blur');
  }
  function setMenuToggleListener() {
    const menuToggle = document.querySelector('.menu-toggle');
    if (!menuToggle) return;
    body = document.querySelector('body');
    menuToggle.addEventListener('click', menuToggleListener);
  }

  hljs.initHighlightingOnLoad();
  setMenuToggleListener();

  InstantClick.on('change', function () {
    document.querySelectorAll('pre code').forEach((block) => {
      hljs.highlightBlock(block);
    });
    setMenuToggleListener();
  });
</script>
</body>
</html>

