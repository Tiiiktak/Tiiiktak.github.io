<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    <title>Pytorch学习笔记_3_构建一个神经网络 - Tiiktak&#39;s</title>
    
    <meta name="description" content="Neural Networks  神经网络可以通过使用torch.nn包来创建
 nn依赖于autograd来定义模型并求导。
 一个nn.Module类包含各个层和一个forward(input)前向传播方法，该方法返回output
  例如这个分类数字图像的网络：
这是个简单的前馈神经网络，它接受一个输入，然后一层接一层的传递，最后输出计算结果
一个神经网络的典型训练过程：
 定义包含一些可学习的参数（或权重）的神经网络 在数据集上迭代 通过神经网络处理输入 计算损失函数（预测值与实际值的差值大小） 将梯度反向传播回网络的参数 更新网络参数，主要使用一个简单的更新法则：weight = weight - learning_rate * gradient   另参见：konosuba.xyz/blog/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4
 定义网络 import torch import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): # 构造方法 super().__init__() # 复制并使用Net的父类的初始化方法，即先运行nn.Module的初始化函数 # 卷积层 self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # fc(full_connect)全连接函数，均为线性函数 y = Wx &#43; b self.">
    <meta name="author" content="">
    
    <link href="https://konosuba.xyz/css/github-gist.min.css" rel="stylesheet">
    <link href="https://konosuba.xyz/css/style.css" rel="stylesheet">
    
    <link rel="apple-touch-icon" href="https://konosuba.xyz/img/apple-touch-icon.png">
    <link rel="icon" href="https://konosuba.xyz/img/favicon.ico">
    
    <meta name="generator" content="Hugo 0.55.6" />
    
    <link rel="alternate" type="application/atom+xml" href="https://konosuba.xyz/index.xml" title="Tiiktak&#39;s">
    
    
    
  </head>
  <body class="single">
    <header class="header">
      <div class="wrap">
        
        <p class="logo"><a href="https://konosuba.xyz/">Tiiktak&#39;s</a></p>
        
        
        <button class="menu-toggle" type="button"></button>
        
      </div>
    </header>
    
    <nav class="nav">
      <ul class="menu">
        
        <li>
          <a href="/about/">About</a>
        </li>
        
      </ul>
    </nav>
    
    <main class="main">


<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">Pytorch学习笔记_3_构建一个神经网络</h1>
    <div class="post-meta">2020.2.12</div>
  </header>
  <div class="post-content">

<h1 id="neural-networks">Neural Networks</h1>

<ul>
<li><p>神经网络可以通过使用<code>torch.nn</code>包来创建</p></li>

<li><p><code>nn</code>依赖于<code>autograd</code>来定义模型并求导。</p></li>

<li><p>一个<code>nn.Module</code>类包含各个层和一个<code>forward(input)</code>前向传播方法，该方法返回<code>output</code></p></li>
</ul>

<p>例如这个分类数字图像的网络：</p>

<p><img src="https://pytorch.org/tutorials/_images/mnist.png" alt="classifies digit images network" /></p>

<p>这是个简单的前馈神经网络，它接受一个输入，然后一层接一层的传递，最后输出计算结果</p>

<p>一个神经网络的典型训练过程：</p>

<ul>
<li>定义包含一些可学习的参数（或权重）的神经网络</li>
<li>在数据集上迭代</li>
<li>通过神经网络处理输入</li>
<li>计算损失函数（预测值与实际值的差值大小）</li>
<li>将梯度反向传播回网络的参数</li>
<li>更新网络参数，主要使用一个简单的更新法则：<code>weight = weight - learning_rate * gradient</code></li>
</ul>

<blockquote>
<p>另参见：<a href="https://konosuba.xyz/blog/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4/">konosuba.xyz/blog/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4</a></p>
</blockquote>

<h1 id="定义网络">定义网络</h1>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    
    def __init__(self): # 构造方法
        super().__init__() # 复制并使用Net的父类的初始化方法，即先运行nn.Module的初始化函数
        
        # 卷积层
        self.conv1 = nn.Conv2d(1, 6, 5) 
        self.conv2 = nn.Conv2d(6, 16, 5) 
        
        # fc(full_connect)全连接函数，均为线性函数 y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120) 
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x): # 前向传播函数
        
        # 将 x 放入卷积层 conv
        # 经过激励函数 ReLu
        # 使用2x2窗口进行最大池化 Max_poolinhg
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # (2, 2)也可直接换作 2

        # view 将 x 展开成一维的向量，总特征数并不改变，为接下来的全连接作准备。
        # view 的作用类似于Numpy中的reshape
        x = x.view(-1, self.num_flat_features(x))

        # 输入x经过 full_connect，再经过ReLU激活函数，然后更新x
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))

        # 输入x经过 full_connect 然后更新x
        x = self.fc3(x)
        return x

    def num_flat_features(self, x): # 计算x的总特征量(把每个数字都看作是一个特征)
                                    # 比如 x 是4*2*2的张量，那么它的特征总量就是16。

        # Pytorch 仅接受批输入（一次性输入多张图片）
        size = x.size()[1:]  # 考虑除了第一个维度以外的所有维度
        
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)
'''
Output:
Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
'''
</code></pre>

<blockquote>
<p><code>super()</code>函数使用参考：<a href="konosuba.xyz">super()</a>
<code>linear()</code>函数使用参考：<a href="konosuba.xyz">linear()</a>
<code>relu</code></p>
</blockquote>

<p><strong>在模型中必须要定义 <code>forward</code> 函数</strong>，<code>backward</code> 函数（用来计算梯度）会被<code>autograd</code>自动创建。</p>

<p>可以在 <code>forward</code> 函数中使用任何针对 <code>Tensor</code> 的操作。</p>

<p><code>net.parameters()</code>返回可被学习的参数（权重）列表和值:</p>

<pre><code class="language-python">params = list(net.parameters())
print(len(params)) 
print(params[0].size())  # conv1's .weight
# Output:
# 10
# torch.Size([6, 1, 5, 5])
</code></pre>

<h1 id="测试网络">测试网络</h1>

<p>测试随机输入32×32。 注：这个网络（LeNet）期望的输入大小是32×32，如果使用MNIST数据集来训练这个网络，请把图片大小重新调整到32×32</p>

<pre><code class="language-python">input = torch.randn(1, 1, 32, 32)
out = net(input)
print(out)
# Output:
# tensor([[-0.1102,  0.0936, -0.0787, -0.0155, -0.0480,  0.0496, -0.0683, -0.0112,
#         -0.0889,  0.0134]], grad_fn=&lt;AddmmBackward&gt;)
</code></pre>

<h1 id="反向传播">反向传播</h1>

<p>将所有参数的梯度缓存清零，然后进行随机梯度的的反向传播：</p>

<pre><code class="language-python">net.zero_grad()
out.backward(torch.randn(1, 10))
</code></pre>

<blockquote>
<p><strong>Note</strong></p>

<p><code>torch.nn</code> 只支持小批量输入。整个 <code>torch.nn</code> 包都只支持小批量样本，而不支持单个样本。</p>

<p>例如，<code>nn.Conv2d</code> 接受一个4维的张量，</p>

<p><code>每一维分别是sSamples * nChannels * Height * Width（样本数*通道数*高*宽）</code>。</p>

<p>如果你有单个样本，只需使用 <code>input.unsqueeze(0)</code> 来添加其它的维数</p>
</blockquote>

<h1 id="损失函数">损失函数</h1>

<p>损失函数接受一对 <code>(output, target)</code> 作为输入来计算一个值以估计网络的输出和目标值相差多少。</p>

<blockquote>
<p><code>output</code>为网络的输出，<code>target</code>为实际值</p>
</blockquote>

<p><code>nn</code>包中有很多不同的损失函数。</p>

<p><code>nn.MSELoss</code>是一个比较简单的损失函数，它计算输出和目标间的均方误差:</p>

<pre><code class="language-python">output = net(input)
target = torch.randn(10)  # 随机值作为样例
target = target.view(1, -1)  # 使target和output的shape相同
criterion = nn.MSELoss()

loss = criterion(output, target)
print(loss)
# Output:
# tensor(1.1103, grad_fn=&lt;MseLossBackward&gt;)
</code></pre>

<p>现在，如果在反向过程中跟随<code>loss</code> ， 使用它的 <code>.grad_fn</code> 属性，将看到如下所示的计算图。</p>

<pre><code>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d
          -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear
          -&gt; MSELoss
          -&gt; loss
</code></pre>

<p>所以，当我们调用 <code>loss.backward()</code>时,整张计算图都会根据<code>loss</code>进行微分，而且图中所有设置为<code>requires_grad=True</code>的张量将会拥有一个随着梯度累积的<code>.grad</code>张量。</p>

<p>为了说明，让我们向后退几步:</p>

<pre><code class="language-python">print(loss.grad_fn)  # MSELoss
print(loss.grad_fn.next_functions[0][0])  # Linear
print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU
# Output:
# &lt;MseLossBackward object at 0x0000021C605F3B08&gt;
# &lt;AddmmBackward object at 0x0000021C605F3208&gt;
# &lt;AccumulateGrad object at 0x0000021C605F3B08&gt;
</code></pre>

<h1 id="反向传播-1">反向传播</h1>

<p>调用<code>loss.backward()</code>获得反向传播的误差。</p>

<p>但是在调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。</p>

<p>现在，我们将调用<code>loss.backward()</code>，并查看<code>conv1</code>层的偏差（bias）项在反向传播前后的梯度。</p>

<pre><code class="language-python">net.zero_grad()     # 清除梯度

print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

loss.backward()

print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)
''' 
Output: 
conv1.bias.grad before backward
tensor([0., 0., 0., 0., 0., 0.])
conv1.bias.grad after backward
tensor([ 0.0242,  0.0145, -0.0015,  0.0144,  0.0084,  0.0309])
'''
</code></pre>

<h1 id="更新权重">更新权重</h1>

<p>在实践中最简单的权重更新规则是随机梯度下降（SGD）：</p>

<pre><code> ``weight = weight - learning_rate * gradient``
</code></pre>

<p>我们可以使用简单的Python代码实现这个规则：</p>

<pre><code class="language-python">
learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
</code></pre>

<p>但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包<code>torch.optim</code>实现了所有的这些规则：</p>

<pre><code class="language-python">import torch.optim as optim

# 创建优化器
optimizer = optim.SGD(net.parameters(), lr=0.01) # learning rate设为0.01

# 在训练循环中
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()    # Does the update
</code></pre>
</div>
  <footer class="post-footer">
    
  </footer>
  
  
  
  <div id="disqus_thread"></div>
  <script>
    var disqus_shortname = 'tiiktak';
    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>
    Please enable JavaScript to view the
    <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  
  
  
</article>

</main>
<footer class="footer">
  <span>&copy; 2020 <a href="https://konosuba.xyz/">Tiiktak&#39;s</a></span>
  <span>&middot;</span>
  <span>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</span>
  <span>&middot;</span>
  <span>Theme️ <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper</a></span>
</footer>
<script src="https://konosuba.xyz/js/instantclick.min.js" data-no-instant></script>
<script data-no-instant>InstantClick.init();</script>
<script src="https://konosuba.xyz/js/highlight.min.js" data-no-instant></script>
<script data-no-instant>
  let body;
  function menuToggleListener() {
    body.classList.toggle('blur');
  }
  function setMenuToggleListener() {
    const menuToggle = document.querySelector('.menu-toggle');
    if (!menuToggle) return;
    body = document.querySelector('body');
    menuToggle.addEventListener('click', menuToggleListener);
  }

  hljs.initHighlightingOnLoad();
  setMenuToggleListener();

  InstantClick.on('change', function () {
    document.querySelectorAll('pre code').forEach((block) => {
      hljs.highlightBlock(block);
    });
    setMenuToggleListener();
  });
</script>
</body>
</html>

