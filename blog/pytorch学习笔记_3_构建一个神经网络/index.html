<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    <title>Pytorch学习笔记_3_构建一个神经网络 - Tiiktak&#39;s</title>
    
    <meta name="description" content="Neural Networks   神经网络可以通过使用torch.nn包来创建
  nn依赖于autograd来定义模型并求导。
  一个nn.Module类包含各个层和一个forward(input)前向传播方法，该方法返回output
  例如这个分类数字图像的网络：
这是个简单的前馈神经网络，它接受一个输入，然后一层接一层的传递，最后输出计算结果
一个神经网络的典型训练过程：
 定义包含一些可学习的参数（或权重）的神经网络 在数据集上迭代 通过神经网络处理输入 计算损失函数（预测值与实际值的差值大小） 将梯度反向传播回网络的参数 更新网络参数，主要使用一个简单的更新法则：weight = weight - learning_rate * gradient   另参见：konosuba.xyz/blog/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4
 定义网络 import torch import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): # 构造方法 super().__init__() # 复制并使用Net的父类的初始化方法，即先运行nn.Module的初始化函数 # 卷积层 self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # fc(full_connect)全连接函数，均为线性函数 y = Wx &#43; b self.">
    <meta name="author" content="">
    
    <link href="https://konosuba.xyz/css/github-gist.min.css" rel="stylesheet">
    <link href="https://konosuba.xyz/css/style.css" rel="stylesheet">
    
    <link rel="apple-touch-icon" href="https://konosuba.xyz/img/apple-touch-icon.png">
    <link rel="icon" href="https://konosuba.xyz/img/favicon.ico">
    
    <meta name="generator" content="Hugo 0.80.0" />
    
    <link rel="alternate" type="application/atom+xml" href="https://konosuba.xyz/index.xml" title="Tiiktak&#39;s">
    
    
    
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141514532-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-141514532-1');
</script>

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?0121c6813b611028681cd70e85594cb6";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>
  </head>
  <body class="single">
    <header class="header">
      <div class="wrap">
        
        <p class="logo"><a href="https://konosuba.xyz/">Tiiktak&#39;s</a></p>
        
        
        <button class="menu-toggle" type="button"></button>
        
      </div>
    </header>
    
    <nav class="nav">
      <ul class="menu">
        
        <li>
          <a href="/about/">About</a>
        </li>
        
      </ul>
    </nav>
    
    <main class="main">


<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">Pytorch学习笔记_3_构建一个神经网络</h1>
    <div class="post-meta">2020.2.12</div>
  </header>
  <div class="post-content"><h1 id="neural-networks">Neural Networks</h1>
<ul>
<li>
<p>神经网络可以通过使用<code>torch.nn</code>包来创建</p>
</li>
<li>
<p><code>nn</code>依赖于<code>autograd</code>来定义模型并求导。</p>
</li>
<li>
<p>一个<code>nn.Module</code>类包含各个层和一个<code>forward(input)</code>前向传播方法，该方法返回<code>output</code></p>
</li>
</ul>
<p>例如这个分类数字图像的网络：</p>
<p><img src="https://pytorch.org/tutorials/_images/mnist.png" alt="classifies digit images network"></p>
<p>这是个简单的前馈神经网络，它接受一个输入，然后一层接一层的传递，最后输出计算结果</p>
<p>一个神经网络的典型训练过程：</p>
<ul>
<li>定义包含一些可学习的参数（或权重）的神经网络</li>
<li>在数据集上迭代</li>
<li>通过神经网络处理输入</li>
<li>计算损失函数（预测值与实际值的差值大小）</li>
<li>将梯度反向传播回网络的参数</li>
<li>更新网络参数，主要使用一个简单的更新法则：<code>weight = weight - learning_rate * gradient</code></li>
</ul>
<blockquote>
<p>另参见：<a href="https://konosuba.xyz/blog/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4/">konosuba.xyz/blog/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4</a></p>
</blockquote>
<h1 id="定义网络">定义网络</h1>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch
<span style="color:#f92672">import</span> torch.nn <span style="color:#f92672">as</span> nn
<span style="color:#f92672">import</span> torch.nn.functional <span style="color:#f92672">as</span> F

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Net</span>(nn<span style="color:#f92672">.</span>Module):
    
    <span style="color:#66d9ef">def</span> __init__(self): <span style="color:#75715e"># 构造方法</span>
        super()<span style="color:#f92672">.</span>__init__() <span style="color:#75715e"># 复制并使用Net的父类的初始化方法，即先运行nn.Module的初始化函数</span>
        
        <span style="color:#75715e"># 卷积层</span>
        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">5</span>) 
        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">5</span>) 
        
        <span style="color:#75715e"># fc(full_connect)全连接函数，均为线性函数 y = Wx + b</span>
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">16</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">120</span>) 
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">120</span>, <span style="color:#ae81ff">84</span>)
        self<span style="color:#f92672">.</span>fc3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">84</span>, <span style="color:#ae81ff">10</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x): <span style="color:#75715e"># 前向传播函数</span>
        
        <span style="color:#75715e"># 将 x 放入卷积层 conv</span>
        <span style="color:#75715e"># 经过激励函数 ReLu</span>
        <span style="color:#75715e"># 使用2x2窗口进行最大池化 Max_poolinhg</span>
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>max_pool2d(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv1(x)), (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>max_pool2d(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv2(x)), <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># (2, 2)也可直接换作 2</span>

        <span style="color:#75715e"># view 将 x 展开成一维的向量，总特征数并不改变，为接下来的全连接作准备。</span>
        <span style="color:#75715e"># view 的作用类似于Numpy中的reshape</span>
        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>num_flat_features(x))

        <span style="color:#75715e"># 输入x经过 full_connect，再经过ReLU激活函数，然后更新x</span>
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc2(x))

        <span style="color:#75715e"># 输入x经过 full_connect 然后更新x</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc3(x)
        <span style="color:#66d9ef">return</span> x

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">num_flat_features</span>(self, x): <span style="color:#75715e"># 计算x的总特征量(把每个数字都看作是一个特征)</span>
                                    <span style="color:#75715e"># 比如 x 是4*2*2的张量，那么它的特征总量就是16。</span>

        <span style="color:#75715e"># Pytorch 仅接受批输入（一次性输入多张图片）</span>
        size <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()[<span style="color:#ae81ff">1</span>:]  <span style="color:#75715e"># 考虑除了第一个维度以外的所有维度</span>
        
        num_features <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
        <span style="color:#66d9ef">for</span> s <span style="color:#f92672">in</span> size:
            num_features <span style="color:#f92672">*=</span> s
        <span style="color:#66d9ef">return</span> num_features


net <span style="color:#f92672">=</span> Net()
<span style="color:#66d9ef">print</span>(net)
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">Output:
</span><span style="color:#e6db74">Net(
</span><span style="color:#e6db74">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
</span><span style="color:#e6db74">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
</span><span style="color:#e6db74">  (fc1): Linear(in_features=400, out_features=120, bias=True)
</span><span style="color:#e6db74">  (fc2): Linear(in_features=120, out_features=84, bias=True)
</span><span style="color:#e6db74">  (fc3): Linear(in_features=84, out_features=10, bias=True)
</span><span style="color:#e6db74">)
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div><blockquote>
<p><code>super()</code>函数使用参考：<a href="konosuba.xyz">super()</a>
<code>linear()</code>函数使用参考：<a href="konosuba.xyz">linear()</a>
<code>relu</code></p>
</blockquote>
<p><strong>在模型中必须要定义 <code>forward</code> 函数</strong>，<code>backward</code> 函数（用来计算梯度）会被<code>autograd</code>自动创建。</p>
<p>可以在 <code>forward</code> 函数中使用任何针对 <code>Tensor</code> 的操作。</p>
<p><code>net.parameters()</code>返回可被学习的参数（权重）列表和值:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">params <span style="color:#f92672">=</span> list(net<span style="color:#f92672">.</span>parameters())
<span style="color:#66d9ef">print</span>(len(params)) 
<span style="color:#66d9ef">print</span>(params[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>size())  <span style="color:#75715e"># conv1&#39;s .weight</span>
<span style="color:#75715e"># Output:</span>
<span style="color:#75715e"># 10</span>
<span style="color:#75715e"># torch.Size([6, 1, 5, 5])</span>
</code></pre></div><h1 id="测试网络">测试网络</h1>
<p>测试随机输入32×32。 注：这个网络（LeNet）期望的输入大小是32×32，如果使用MNIST数据集来训练这个网络，请把图片大小重新调整到32×32</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>)
out <span style="color:#f92672">=</span> net(input)
<span style="color:#66d9ef">print</span>(out)
<span style="color:#75715e"># Output:</span>
<span style="color:#75715e"># tensor([[-0.1102,  0.0936, -0.0787, -0.0155, -0.0480,  0.0496, -0.0683, -0.0112,</span>
<span style="color:#75715e">#         -0.0889,  0.0134]], grad_fn=&lt;AddmmBackward&gt;)</span>
</code></pre></div><h1 id="反向传播">反向传播</h1>
<p>将所有参数的梯度缓存清零，然后进行随机梯度的的反向传播：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">net<span style="color:#f92672">.</span>zero_grad()
out<span style="color:#f92672">.</span>backward(torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">10</span>))
</code></pre></div><blockquote>
<p><strong>Note</strong></p>
</blockquote>
<blockquote>
<p><code>torch.nn</code> 只支持小批量输入。整个 <code>torch.nn</code> 包都只支持小批量样本，而不支持单个样本。</p>
</blockquote>
<blockquote>
<p>例如，<code>nn.Conv2d</code> 接受一个4维的张量，</p>
</blockquote>
<blockquote>
<p><code>每一维分别是sSamples * nChannels * Height * Width（样本数*通道数*高*宽）</code>。</p>
</blockquote>
<blockquote>
<p>如果你有单个样本，只需使用 <code>input.unsqueeze(0)</code> 来添加其它的维数</p>
</blockquote>
<h1 id="损失函数">损失函数</h1>
<p>损失函数接受一对 <code>(output, target)</code> 作为输入来计算一个值以估计网络的输出和目标值相差多少。</p>
<blockquote>
<p><code>output</code>为网络的输出，<code>target</code>为实际值</p>
</blockquote>
<p><code>nn</code>包中有很多不同的损失函数。</p>
<p><code>nn.MSELoss</code>是一个比较简单的损失函数，它计算输出和目标间的均方误差:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">output <span style="color:#f92672">=</span> net(input)
target <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">10</span>)  <span style="color:#75715e"># 随机值作为样例</span>
target <span style="color:#f92672">=</span> target<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># 使target和output的shape相同</span>
criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MSELoss()

loss <span style="color:#f92672">=</span> criterion(output, target)
<span style="color:#66d9ef">print</span>(loss)
<span style="color:#75715e"># Output:</span>
<span style="color:#75715e"># tensor(1.1103, grad_fn=&lt;MseLossBackward&gt;)</span>
</code></pre></div><p>现在，如果在反向过程中跟随<code>loss</code> ， 使用它的 <code>.grad_fn</code> 属性，将看到如下所示的计算图。</p>
<pre><code>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d
          -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear
          -&gt; MSELoss
          -&gt; loss
</code></pre><p>所以，当我们调用 <code>loss.backward()</code>时,整张计算图都会根据<code>loss</code>进行微分，而且图中所有设置为<code>requires_grad=True</code>的张量将会拥有一个随着梯度累积的<code>.grad</code>张量。</p>
<p>为了说明，让我们向后退几步:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">print</span>(loss<span style="color:#f92672">.</span>grad_fn)  <span style="color:#75715e"># MSELoss</span>
<span style="color:#66d9ef">print</span>(loss<span style="color:#f92672">.</span>grad_fn<span style="color:#f92672">.</span>next_functions[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>])  <span style="color:#75715e"># Linear</span>
<span style="color:#66d9ef">print</span>(loss<span style="color:#f92672">.</span>grad_fn<span style="color:#f92672">.</span>next_functions[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>next_functions[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>])  <span style="color:#75715e"># ReLU</span>
<span style="color:#75715e"># Output:</span>
<span style="color:#75715e"># &lt;MseLossBackward object at 0x0000021C605F3B08&gt;</span>
<span style="color:#75715e"># &lt;AddmmBackward object at 0x0000021C605F3208&gt;</span>
<span style="color:#75715e"># &lt;AccumulateGrad object at 0x0000021C605F3B08&gt;</span>
</code></pre></div><h1 id="反向传播-1">反向传播</h1>
<p>调用<code>loss.backward()</code>获得反向传播的误差。</p>
<p>但是在调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。</p>
<p>现在，我们将调用<code>loss.backward()</code>，并查看<code>conv1</code>层的偏差（bias）项在反向传播前后的梯度。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">net<span style="color:#f92672">.</span>zero_grad()     <span style="color:#75715e"># 清除梯度</span>

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;conv1.bias.grad before backward&#39;</span>)
<span style="color:#66d9ef">print</span>(net<span style="color:#f92672">.</span>conv1<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>grad)

loss<span style="color:#f92672">.</span>backward()

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;conv1.bias.grad after backward&#39;</span>)
<span style="color:#66d9ef">print</span>(net<span style="color:#f92672">.</span>conv1<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>grad)
<span style="color:#e6db74">&#39;&#39;&#39; 
</span><span style="color:#e6db74">Output: 
</span><span style="color:#e6db74">conv1.bias.grad before backward
</span><span style="color:#e6db74">tensor([0., 0., 0., 0., 0., 0.])
</span><span style="color:#e6db74">conv1.bias.grad after backward
</span><span style="color:#e6db74">tensor([ 0.0242,  0.0145, -0.0015,  0.0144,  0.0084,  0.0309])
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div><h1 id="更新权重">更新权重</h1>
<p>在实践中最简单的权重更新规则是随机梯度下降（SGD）：</p>
<pre><code> ``weight = weight - learning_rate * gradient``
</code></pre>
<p>我们可以使用简单的Python代码实现这个规则：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
<span style="color:#66d9ef">for</span> f <span style="color:#f92672">in</span> net<span style="color:#f92672">.</span>parameters():
    f<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>sub_(f<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>data <span style="color:#f92672">*</span> learning_rate)
</code></pre></div><p>但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包<code>torch.optim</code>实现了所有的这些规则：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch.optim <span style="color:#f92672">as</span> optim

<span style="color:#75715e"># 创建优化器</span>
optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(net<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>) <span style="color:#75715e"># learning rate设为0.01</span>

<span style="color:#75715e"># 在训练循环中</span>
optimizer<span style="color:#f92672">.</span>zero_grad()   <span style="color:#75715e"># zero the gradient buffers</span>
output <span style="color:#f92672">=</span> net(input)
loss <span style="color:#f92672">=</span> criterion(output, target)
loss<span style="color:#f92672">.</span>backward()
optimizer<span style="color:#f92672">.</span>step()    <span style="color:#75715e"># Does the update</span>
</code></pre></div></div>
  <footer class="post-footer">
    
  </footer>
  
  
  
  <div id="disqus_thread"></div>
  <script>
    var disqus_shortname = 'tiiktak';
    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>
    Please enable JavaScript to view the
    <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  
  
  
</article>

</main>
<footer class="footer">
  <span>&copy; 2021 <a href="https://konosuba.xyz/">Tiiktak&#39;s</a></span>
  <span>&middot;</span>
  <span>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</span>
  <span>&middot;</span>
  <span>Theme️ <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper</a></span>
</footer>
<script src="https://konosuba.xyz/js/instantclick.min.js" data-no-instant></script>
<script data-no-instant>InstantClick.init();</script>
<script src="https://konosuba.xyz/js/highlight.min.js" data-no-instant></script>
<script data-no-instant>
  let body;
  function menuToggleListener() {
    body.classList.toggle('blur');
  }
  function setMenuToggleListener() {
    const menuToggle = document.querySelector('.menu-toggle');
    if (!menuToggle) return;
    body = document.querySelector('body');
    menuToggle.addEventListener('click', menuToggleListener);
  }

  hljs.initHighlightingOnLoad();
  setMenuToggleListener();

  InstantClick.on('change', function () {
    document.querySelectorAll('pre code').forEach((block) => {
      hljs.highlightBlock(block);
    });
    setMenuToggleListener();
  });
</script>
</body>
</html>

