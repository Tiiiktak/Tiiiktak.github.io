<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    <title>Pytorch中的激活函数 - Tiiktak&#39;s</title>
    
    <meta name="description" content="介绍神经网络的时候已经说到，神经元会对化学物质的刺激进行，当达到一定程度的时候，神经元才会兴奋，并向其他神经元发送信息。神经网络中的激活函数就是用来判断我们所计算的信息是否达到了往后面传输的条件。
为什么激活函数都是非线性的 因为如果使用线性的激活函数，那么input跟output之间的关系始终为线性的，这样完全可以不使用网络结构，直接使用线性组合即可。
所以需要激活函数来引入非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中，增加了神经网络模型泛化的特性。
一般只有在输出层有极小的可能性使用线性激活函数，在隐含层都使用非线性激活函数.
常见的激活函数 # 初始化一些信息 import torch import torch.nn.functional as F import matplotlib.pyplot as plt import numpy as np x= torch.linspace(-10,10,60) Sigmoid 函数 g(z) = a = 1 / (1 &#43; e^(-z)) g&#39;(z) = a&#39; = a (1 - a) 在sigmod函数中我们可以看到，其输出是在(0,1)这个开区间，它能够把输入的连续实值变换为0和1之间的输出，如果是非常大的负数，那么输出就是0；如果是非常大的正数输出就是1，起到了抑制的作用。
ax = plt.gca() ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.xaxis.set_ticks_position(&#39;bottom&#39;) ax.spines[&#39;bottom&#39;].set_position((&#39;data&#39;, 0)) ax.yaxis.set_ticks_position(&#39;left&#39;) ax.spines[&#39;left&#39;].set_position((&#39;data&#39;, 0)) plt.ylim((0, 1)) sigmod=torch.sigmoid(x) plt.plot(x.numpy(),sigmod.numpy()) 但是sigmod由于需要进行指数运算（这个对于计算机来说是比较慢，相比relu），再加上函数输出不是以0为中心的（这样会使权重更新效率降低），当输入稍微远离了坐标原点，函数的梯度就变得很小了（几乎为零）。
在神经网络反向传播的过程中不利于权重的优化，这个问题叫做梯度饱和，也可以叫梯度弥散。这些不足，所以现在使用到sigmod基本很少了，基本上只有在做二元分类（0，1）时的输出层才会使用。
Tanh 函数 tanh是双曲正切函数，输出区间是在(-1,1)之间，而且整个函数是以0为中心的
ax = plt.gca() ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.xaxis.set_ticks_position(&#39;bottom&#39;) ax.">
    <meta name="author" content="">
    
    <link href="https://konosuba.xyz/css/github-gist.min.css" rel="stylesheet">
    <link href="https://konosuba.xyz/css/style.css" rel="stylesheet">
    
    <link rel="apple-touch-icon" href="https://konosuba.xyz/img/apple-touch-icon.png">
    <link rel="icon" href="https://konosuba.xyz/img/favicon.ico">
    
    <meta name="generator" content="Hugo 0.80.0" />
    
    <link rel="alternate" type="application/atom+xml" href="https://konosuba.xyz/index.xml" title="Tiiktak&#39;s">
    
    
    
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141514532-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-141514532-1');
</script>

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?0121c6813b611028681cd70e85594cb6";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>
  </head>
  <body class="single">
    <header class="header">
      <div class="wrap">
        
        <p class="logo"><a href="https://konosuba.xyz/">Tiiktak&#39;s</a></p>
        
        
        <button class="menu-toggle" type="button"></button>
        
      </div>
    </header>
    
    <nav class="nav">
      <ul class="menu">
        
        <li>
          <a href="/about/">About</a>
        </li>
        
      </ul>
    </nav>
    
    <main class="main">


<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">Pytorch中的激活函数</h1>
    <div class="post-meta">2020.2.15</div>
  </header>
  <div class="post-content"><p>介绍神经网络的时候已经说到，神经元会对化学物质的刺激进行，当达到一定程度的时候，神经元才会兴奋，并向其他神经元发送信息。神经网络中的激活函数就是用来判断我们所计算的信息是否达到了往后面传输的条件。</p>
<h1 id="为什么激活函数都是非线性的">为什么激活函数都是非线性的</h1>
<p>因为如果使用线性的激活函数，那么input跟output之间的关系始终为线性的，这样完全可以不使用网络结构，直接使用线性组合即可。</p>
<p>所以需要<strong>激活函数来引入非线性因素</strong>，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中，增加了神经网络模型泛化的特性。</p>
<p>一般只有在输出层有极小的可能性使用线性激活函数，在隐含层都使用非线性激活函数.</p>
<h1 id="常见的激活函数">常见的激活函数</h1>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># 初始化一些信息</span>
<span style="color:#f92672">import</span> torch
<span style="color:#f92672">import</span> torch.nn.functional <span style="color:#f92672">as</span> F
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
x<span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>linspace(<span style="color:#f92672">-</span><span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">60</span>)
</code></pre></div><h2 id="sigmoid-函数">Sigmoid 函数</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">g(z) <span style="color:#f92672">=</span> a <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> e<span style="color:#f92672">^</span>(<span style="color:#f92672">-</span>z))
g<span style="color:#e6db74">&#39;(z) = a&#39;</span> <span style="color:#f92672">=</span> a (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> a)
</code></pre></div><p>在sigmod函数中我们可以看到，其输出是在(0,1)这个开区间，它能够把输入的连续实值变换为0和1之间的输出，如果是非常大的负数，那么输出就是0；如果是非常大的正数输出就是1，起到了抑制的作用。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>gca()
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;right&#39;</span>]<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;none&#39;</span>)
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;top&#39;</span>]<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;none&#39;</span>)
ax<span style="color:#f92672">.</span>xaxis<span style="color:#f92672">.</span>set_ticks_position(<span style="color:#e6db74">&#39;bottom&#39;</span>)
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;bottom&#39;</span>]<span style="color:#f92672">.</span>set_position((<span style="color:#e6db74">&#39;data&#39;</span>, <span style="color:#ae81ff">0</span>))
ax<span style="color:#f92672">.</span>yaxis<span style="color:#f92672">.</span>set_ticks_position(<span style="color:#e6db74">&#39;left&#39;</span>)
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;left&#39;</span>]<span style="color:#f92672">.</span>set_position((<span style="color:#e6db74">&#39;data&#39;</span>, <span style="color:#ae81ff">0</span>))
plt<span style="color:#f92672">.</span>ylim((<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>))
sigmod<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>sigmoid(x)
plt<span style="color:#f92672">.</span>plot(x<span style="color:#f92672">.</span>numpy(),sigmod<span style="color:#f92672">.</span>numpy())
</code></pre></div><p><img src="https://i.loli.net/2020/02/16/RKB5qhxYMDgkIWT.png" alt=""></p>
<p>但是sigmod由于需要进行指数运算（这个对于计算机来说是比较慢，相比relu），再加上函数输出<em>不是以0为中心</em>的（这样会使权重更新效率降低），当输入稍微远离了坐标原点，函数的梯度就变得很小了（几乎为零）。</p>
<p>在神经网络反向传播的过程中不利于权重的优化，这个问题叫做<strong>梯度饱和</strong>，也可以叫梯度弥散。这些不足，所以现在使用到sigmod基本很少了，基本上只有在做二元分类（0，1）时的输出层才会使用。</p>
<h2 id="tanh-函数">Tanh 函数</h2>
<p><img src="https://i.loli.net/2020/02/16/7VAq3RDG9lcJTei.png" alt=""></p>
<p>tanh是双曲正切函数，输出区间是在(-1,1)之间，而且整个函数是<em>以0为中心</em>的</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>gca()
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;right&#39;</span>]<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;none&#39;</span>)
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;top&#39;</span>]<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;none&#39;</span>)
ax<span style="color:#f92672">.</span>xaxis<span style="color:#f92672">.</span>set_ticks_position(<span style="color:#e6db74">&#39;bottom&#39;</span>)
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;bottom&#39;</span>]<span style="color:#f92672">.</span>set_position((<span style="color:#e6db74">&#39;data&#39;</span>, <span style="color:#ae81ff">0</span>))
ax<span style="color:#f92672">.</span>yaxis<span style="color:#f92672">.</span>set_ticks_position(<span style="color:#e6db74">&#39;left&#39;</span>)
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;left&#39;</span>]<span style="color:#f92672">.</span>set_position((<span style="color:#e6db74">&#39;data&#39;</span>, <span style="color:#ae81ff">0</span>))
plt<span style="color:#f92672">.</span>ylim((<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>))
tanh<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>tanh(x)
plt<span style="color:#f92672">.</span>plot(x<span style="color:#f92672">.</span>numpy(),tanh<span style="color:#f92672">.</span>numpy())
</code></pre></div><p><img src="https://i.loli.net/2020/02/16/PhFgnitJ5D82T1z.png" alt=""></p>
<p>与sigmoid函数类似，当输入稍微远离了坐标原点，梯度还是会很小，但是好在tanh是以0为中心点，如果使用tanh作为激活函数，还能起到归一化（均值为0）的效果。</p>
<p>一般二分类问题中，隐藏层用tanh函数，输出层用sigmod函数，但是随着Relu的出现所有的隐藏层基本上都使用relu来作为激活函数了</p>
<h2 id="relu-函数">ReLu 函数</h2>
<blockquote>
<p>Relu(Rectified Linear Units) 修正线性单元</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">a <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0</span>, z)
</code></pre></div><ul>
<li>当 z &gt; 0 时，梯度始终为1，从而提高神经网络基于梯度算法的运算速度。</li>
<li>当 z &lt; 0 时，梯度始终为0</li>
</ul>
<p>ReLu函数只有线性关系（只需要判断输入是否大于0），不管是前向传播还是反向传播，都比Simoid和Tanh要快很多</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>gca()
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;rights&#39;</span>]<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;none&#39;</span>)
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;top&#39;</span>]<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;none&#39;</span>)
ax<span style="color:#f92672">.</span>xaxis<span style="color:#f92672">.</span>set_ticks_position(<span style="color:#e6db74">&#39;bottom&#39;</span>)
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;bottom&#39;</span>]<span style="color:#f92672">.</span>set_position((<span style="color:#e6db74">&#39;data&#39;</span>, <span style="color:#ae81ff">0</span>))
ax<span style="color:#f92672">.</span>yaxis<span style="color:#f92672">.</span>set_ticks_position((<span style="color:#e6db74">&#39;data&#39;</span>, <span style="color:#ae81ff">0</span>))
plt<span style="color:#f92672">.</span>ylim((<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">10</span>))
relu <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(x)
plt<span style="color:#f92672">.</span>plot(x<span style="color:#f92672">.</span>numpy(), relu<span style="color:#f92672">.</span>numpy())
</code></pre></div><p><img src="https://i.loli.net/2020/02/16/zg8UuBpocDdXIfS.png" alt=""></p>
<p>当输入是负数的时候，ReLU是完全不被激活的；但是到了反向传播过程中，输入负数，梯度就会完全到0，这个和sigmod函数、tanh函数有一样的问题。 实际的运用中，该缺陷的影响不是很大。</p>
<h2 id="leaky-relu-函数">Leaky Relu 函数</h2>
<p>为了解决relu函数z&lt;0时的问题出现了 Leaky ReLU函数，该函数保证在<em>z&lt;0的时候，梯度仍然不为0</em>。</p>
<p>ReLU的前半段设为αz而非0，通常α=0.01</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">a <span style="color:#f92672">=</span> max(<span style="color:#960050;background-color:#1e0010">𝛼</span>z, z)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>gca()
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;right&#39;</span>]<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;none&#39;</span>)
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;top&#39;</span>]<span style="color:#f92672">.</span>set_color(<span style="color:#e6db74">&#39;none&#39;</span>)
ax<span style="color:#f92672">.</span>xaxis<span style="color:#f92672">.</span>set_ticks_position(<span style="color:#e6db74">&#39;bottom&#39;</span>)
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;bottom&#39;</span>]<span style="color:#f92672">.</span>set_position((<span style="color:#e6db74">&#39;data&#39;</span>, <span style="color:#ae81ff">0</span>))
ax<span style="color:#f92672">.</span>yaxis<span style="color:#f92672">.</span>set_ticks_position(<span style="color:#e6db74">&#39;left&#39;</span>)
ax<span style="color:#f92672">.</span>spines[<span style="color:#e6db74">&#39;left&#39;</span>]<span style="color:#f92672">.</span>set_position((<span style="color:#e6db74">&#39;data&#39;</span>, <span style="color:#ae81ff">0</span>))
plt<span style="color:#f92672">.</span>ylim((<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">10</span>))
l_relu<span style="color:#f92672">=</span>F<span style="color:#f92672">.</span>leaky_relu(x,<span style="color:#ae81ff">0.1</span>) <span style="color:#75715e"># 这里的0.1是为了方便展示，理论上应为0.01甚至更小的值</span>
plt<span style="color:#f92672">.</span>plot(x<span style="color:#f92672">.</span>numpy(),l_relu<span style="color:#f92672">.</span>numpy())
</code></pre></div><p><img src="https://i.loli.net/2020/02/16/VHhZr9bgknjupLB.png" alt=""></p>
<p>理论上来讲，Leaky ReLU有ReLU的所有优点，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。</p>
<p>ReLU目前仍是最常用的activation function，在隐藏层中推荐优先尝试！</p>
</div>
  <footer class="post-footer">
    
  </footer>
  
  
  
  <div id="disqus_thread"></div>
  <script>
    var disqus_shortname = 'tiiktak';
    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>
    Please enable JavaScript to view the
    <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  
  
  
</article>

</main>
<footer class="footer">
  <span>&copy; 2021 <a href="https://konosuba.xyz/">Tiiktak&#39;s</a></span>
  <span>&middot;</span>
  <span>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</span>
  <span>&middot;</span>
  <span>Theme️ <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper</a></span>
</footer>
<script src="https://konosuba.xyz/js/instantclick.min.js" data-no-instant></script>
<script data-no-instant>InstantClick.init();</script>
<script src="https://konosuba.xyz/js/highlight.min.js" data-no-instant></script>
<script data-no-instant>
  let body;
  function menuToggleListener() {
    body.classList.toggle('blur');
  }
  function setMenuToggleListener() {
    const menuToggle = document.querySelector('.menu-toggle');
    if (!menuToggle) return;
    body = document.querySelector('body');
    menuToggle.addEventListener('click', menuToggleListener);
  }

  hljs.initHighlightingOnLoad();
  setMenuToggleListener();

  InstantClick.on('change', function () {
    document.querySelectorAll('pre code').forEach((block) => {
      hljs.highlightBlock(block);
    });
    setMenuToggleListener();
  });
</script>
</body>
</html>

